{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d78e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2933f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mano/anaconda3/envs/torchenv/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41208a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b0f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneralDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Ideally, should work with any dataset.\n",
    "    Just pass the inference lines to the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # create attribute for all kwargs\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def __len__(self):\n",
    "        for k in self.__dict__:\n",
    "            return len(self.__dict__[k])\n",
    "        return -1\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # print all attributes\n",
    "        return dict((k, self.__dict__[k][item]) for k in self.__dict__)\n",
    "\n",
    "def generate_sample(model, tokenizer, dataloader, device):\n",
    "    \"\"\"\n",
    "    generate negative samples using the model for revise training\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    model = model.module if hasattr(model, \"module\") else model\n",
    "    model.eval()\n",
    "    beam_size = 10\n",
    "    samples_list = []\n",
    "    for data in tqdm(dataloader):\n",
    "        prob, label = data[\"prob\"], data[\"label\"]\n",
    "        gen_prob = prob\n",
    "        batch = tokenizer.prepare_seq2seq_batch(gen_prob, return_tensors=\"pt\")\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        text = model.generate(\n",
    "            **batch,\n",
    "            num_beams=beam_size,\n",
    "            early_stopping=True,\n",
    "            max_length=64,\n",
    "            num_return_sequences=beam_size,\n",
    "        )  # batch * 10, len\n",
    "        text = tokenizer.batch_decode(text, skip_special_tokens=True)\n",
    "        text = [clean_text(t) for t in text]\n",
    "\n",
    "        label = [clean_text(t) for t in label]\n",
    "        \n",
    "        idx = 0\n",
    "\n",
    "\n",
    "        for p, e in zip(prob, label):\n",
    "            local_samples_list = dict()\n",
    "            local_samples_list[\"prob\"] = p\n",
    "            local_samples_list[\"equations\"] = []\n",
    "            local_samples_list[\"gt\"] = []\n",
    "            local_samples_list[\"correct_answer\"] = e\n",
    "\n",
    "            samples.append((p, \"<mask>\", e, 0))\n",
    "            samples.append((p, e, e, 1))\n",
    "            beam = text[idx * beam_size : (idx + 1) * beam_size]\n",
    "            for b in beam:\n",
    "                if is_equal(e, b, number_filler=True):\n",
    "                    samples.append((p, b, b, 1))\n",
    "                    local_samples_list[\"equations\"].append(b)\n",
    "                    local_samples_list[\"gt\"].append(1)\n",
    "                else:\n",
    "                    samples.append((p, b, e, 0))\n",
    "                    local_samples_list[\"equations\"].append(b)\n",
    "                    local_samples_list[\"gt\"].append(0)\n",
    "            \n",
    "            samples_list.append(local_samples_list)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    return samples_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3859a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare train data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 151500.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from debugmodels/svamp_t5_batch_output/generator_Mar_03_2023_svamp_infix/saved_model/\n",
      "model load done\n",
      "Total parameters: 1476393986\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"t5_codet5_based/\")\n",
    "\n",
    "from t5_GenerateRankModel import MyT5ForSequenceClassificationAndGeneration\n",
    "from transformers import T5Config, T5Tokenizer\n",
    "from utils import read_json, clean_text, is_equal\n",
    "from tqdm import tqdm\n",
    "from data_utils import extract_text_label\n",
    "\n",
    "\n",
    "# open a json file and read it where text is in \"text\" key and infix equation is in \"template_equ\" key\n",
    "train_file = \"data/mawps_asdiv-a_svamp/testset_nodup.json\"\n",
    "data_limit = -1\n",
    "batch_size = 16\n",
    "eqn_order = \"infix\"\n",
    "model_path = \"debugmodels/svamp_t5_batch_output/generator_Mar_03_2023_svamp_infix/saved_model/\"\n",
    "\n",
    "\n",
    "data = read_json(train_file)\n",
    "lines = []\n",
    "labels = []\n",
    "numbers_list = []\n",
    "for i, item in enumerate(tqdm(data, desc=\"Prepare train data\")):\n",
    "    goal, proof, numbers = extract_text_label(item, eqn_order)\n",
    "    lines.append(goal)\n",
    "    labels.append(proof)\n",
    "    numbers_list.append(numbers)\n",
    "    if data_limit > 0 and i > data_limit:\n",
    "        break\n",
    "raw_train_dataset = GeneralDataset(\n",
    "    prob=lines, label=labels, numbers=numbers_list\n",
    ")\n",
    "\n",
    "extra_args = {}\n",
    "raw_train_dataloader = torch.utils.data.DataLoader(\n",
    "    raw_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    **extra_args,\n",
    "    )\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    model_path, do_lower_case=False\n",
    ")\n",
    "\n",
    "print(f\"load model from {model_path}\")\n",
    "config = T5Config.from_pretrained(model_path)\n",
    "config.num_labels = 2\n",
    "config.id2label = {\"0\": \"LABEL_0\", \"1\": \"LABEL_1\"}\n",
    "config.label2id = {\"LABEL_0\": 0, \"LABEL_1\": 1}\n",
    "\n",
    "model = MyT5ForSequenceClassificationAndGeneration(\n",
    "    modelpath= model_path, config=config, d_model=config.d_model, num_labels=2\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"model load done\")\n",
    "\n",
    "config = model.config\n",
    "model.to(device)\n",
    "\n",
    "# model size\n",
    "size = 0\n",
    "for n, p in model.named_parameters():\n",
    "    size += p.nelement()\n",
    "print(\"Total parameters: {}\".format(size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581823fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start generate samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/63 [00:00<?, ?it/s]/home/mano/anaconda3/envs/torchenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "  2%|██▋                                                                                                                                                                    | 1/63 [00:24<25:20, 24.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart generate samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m gen_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m samples_ans \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish generate samples in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mgen_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mgenerate_sample\u001b[0;34m(model, tokenizer, dataloader, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     35\u001b[0m     batch[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 37\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batch * 10, len\u001b[39;00m\n\u001b[1;32m     44\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(text, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m text \u001b[38;5;241m=\u001b[39m [clean_text(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text]\n",
      "File \u001b[0;32m~/Desktop/emlct/y2/thesis/code/noah-research/NLP/GenerateRank/t5_codet5_based/t5_GenerateRankModel.py:77\u001b[0m, in \u001b[0;36mMyT5ForSequenceClassificationAndGeneration.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# call generate function of the parent class T5ForConditionalGeneration\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/transformers/generation_utils.py:1456\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1453\u001b[0m         input_ids, expand_size\u001b[38;5;241m=\u001b[39mnum_beams, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1454\u001b[0m     )\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;66;03m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1470\u001b[0m     \u001b[38;5;66;03m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1472\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1473\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         renormalize_logits\u001b[38;5;241m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1478\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/transformers/generation_utils.py:2373\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2370\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2371\u001b[0m )\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2373\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   2376\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration._reorder_cache\u001b[0;34m(self, past, beam_idx)\u001b[0m\n\u001b[1;32m   1742\u001b[0m reordered_layer_past_states \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past_state \u001b[38;5;129;01min\u001b[39;00m layer_past_states:\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;66;03m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m     reordered_layer_past_states \u001b[38;5;241m=\u001b[39m reordered_layer_past_states \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m-> 1746\u001b[0m         \u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1747\u001b[0m     )\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m reordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reordered_layer_past_states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(layer_past_states)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# generate samples for revise\n",
    "\n",
    "print(\"start generate samples\")\n",
    "gen_start_time = time.time()\n",
    "samples_ans = generate_sample(\n",
    "    model, tokenizer, raw_train_dataloader, device\n",
    ")\n",
    "print(f\"finish generate samples in {time.time() - gen_start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef7272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c57a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"t5_preds.json\", \"w\") as fh:\n",
    "    json.dump(samples_ans, fh, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3f758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce6697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
